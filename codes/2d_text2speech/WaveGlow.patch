--- /home/dcg-adlr-rafaelvalle-source.cosmos597/repos/nvidia/waveglow/glow.py
+++ /home/dcg-adlr-rafaelvalle-source.cosmos597/repos/nvidia/waveglow/glow.py
@@ -50,15 +50,15 @@
 
         for k in range(self.n_flows):
             if k % self.n_early_every == 0 and k > 0:
-                output_audio.append(audio[:, :self.n_early_size, :])
-                audio = audio[:, self.n_early_size:, :]
+                output_audio.append(audio[:,:self.n_early_size,:])
+                audio = audio[:,self.n_early_size:,:]
 
             audio, log_det_W = self.convinv[k](audio)
             log_det_W_list.append(log_det_W)
 
             n_half = int(audio.size(1)/2)
-            audio_0 = audio[:, :n_half, :]
-            audio_1 = audio[:, n_half:, :]
+            audio_0 = audio[:,:n_half,:]
+            audio_1 = audio[:,n_half:,:]
 
             output = self.WN[k]((audio_0, spect))
             log_s = output[:, n_half:, :]
@@ -66,10 +66,10 @@
             audio_1 = torch.exp(log_s)*audio_1 + b
             log_s_list.append(log_s)
 
-            audio = torch.cat([audio_0, audio_1], 1)
+            audio = torch.cat([audio_0, audio_1],1)
 
         output_audio.append(audio)
-        return torch.cat(output_audio, 1), log_s_list, log_det_W_list
+        return torch.cat(output_audio,1), log_s_list, log_det_W_list
 
     def infer(self, spect, sigma=1.0):
         spect = self.upsample(spect)
@@ -93,28 +93,25 @@
 
         for k in reversed(range(self.n_flows)):
             n_half = int(audio.size(1)/2)
-            audio_0 = audio[:, :n_half, :]
-            audio_1 = audio[:, n_half:, :]
+            audio_0 = audio[:,:n_half,:]
+            audio_1 = audio[:,n_half:,:]
 
             output = self.WN[k]((audio_0, spect))
             s = output[:, n_half:, :]
             b = output[:, :n_half, :]
             audio_1 = (audio_1 - b)/torch.exp(s)
-            audio = torch.cat([audio_0, audio_1], 1)
+            audio = torch.cat([audio_0, audio_1],1)
 
             audio = self.convinv[k](audio, reverse=True)
 
             if k % self.n_early_every == 0 and k > 0:
                 if spect.type() == 'torch.cuda.HalfTensor':
-                    z = torch.cuda.HalfTensor(spect.size(
-                        0), self.n_early_size, spect.size(2)).normal_()
+                    z = torch.cuda.HalfTensor(spect.size(0), self.n_early_size, spect.size(2)).normal_()
                 else:
-                    z = torch.cuda.FloatTensor(spect.size(
-                        0), self.n_early_size, spect.size(2)).normal_()
-                audio = torch.cat((sigma*z, audio), 1)
+                    z = torch.cuda.FloatTensor(spect.size(0), self.n_early_size, spect.size(2)).normal_()
+                audio = torch.cat((sigma*z, audio),1)
 
-        audio = audio.permute(0, 2, 1).contiguous().view(
-            audio.size(0), -1).data
+        audio = audio.permute(0,2,1).contiguous().view(audio.size(0), -1).data
         return audio
 
     @staticmethod